################################################
### CONFIGURATION FILE FOR AN SMT EXPERIMENT ###
################################################

[GENERAL]

###### you need to set these paths to match your environemnt
######

moses-src-dir = /home/rsennrich/tools/mosesdecoder
wmt2014-scripts = /home/rsennrich/smtworkspace/wmt2014-scripts
parzu-path = /home/rsennrich/ParZu # https://github.com/rsennrich/ParZu
zmorge-model = /home/rsennrich/zmorge/zmorge-20141224-smor_newlemma.a #get this (or a newer version) from http://kitt.ifi.uzh.ch/kitt/zmorge/
srilm-dir = /home/rsennrich/tools/srilm/bin/i686-m64/
external-bin-dir = ~/bin

###### no further changes should be required to run the toy example
###### (but feel free to experiment with different settings, or change the training/test data)

moses-script-dir = $moses-src-dir/scripts
moses-bin-dir = $moses-src-dir/bin
toy-data = $wmt2014-scripts/example/data
working-dir = $wmt2014-scripts/example/working-dir
decoder = $moses-src-dir/bin/moses

input-tokenizer = "$moses-script-dir/tokenizer/normalize-punctuation.perl $input-extension | $moses-script-dir/tokenizer/tokenizer.perl -l $input-extension -penn"
output-tokenizer = "$moses-script-dir/tokenizer/normalize-punctuation.perl $output-extension | $moses-script-dir/tokenizer/tokenizer.perl -l $output-extension"
input-truecaser = $moses-script-dir/recaser/truecase.perl
output-truecaser = $moses-script-dir/recaser/truecase.perl
detruecaser = $moses-script-dir/recaser/detruecase.perl

# parsing pipeline used for WMT 2014
output-parser = "$moses-script-dir/tokenizer/deescape-special-chars.perl | $parzu-path/parzu -i tokenized_lines --projective | $wmt2014-scripts/enrich_labelset.py --wmt15 | $moses-script-dir/training/wrappers/conll2mosesxml.py"

# also parse tuning/evaluation reference files
mock-output-parser-references = $output-parser

# head binarization
output-parse-relaxer = "$wmt2014-scripts/emnlp2015/binarize.py head"

# hybrid compound splitting (described in Sennrich, Williams and Huck, 2015)
output-splitter = "$wmt2014-scripts/hybrid_compound_splitter.py -smor $zmorge-model -write-filler -no-truecase -q -syntax -dependency -fewest"

# sed instructions unsplit the split compunds from output-splitter
detokenizer = "$moses-script-dir/tokenizer/detokenizer.perl -l $output-extension | sed -r 's/ \@(\S*?)\@ /\1/g' | sed -r 's/\@\@ //g'"

input-extension = en
output-extension = de
pair-extension = de-en

generic-parallelizer = $moses-script-dir/ems/support/generic-multicore-parallelizer.perl

jobs = 10

#################################################################
# PARALLEL CORPUS PREPARATION: 
# create a tokenized, sentence-aligned corpus, ready for training

[CORPUS]

cores = 10

### tools to use to prepare the data
#
#tokenizer = 
#lowercaser = 

### long sentences are filtered out, since they slow down GIZA++ 
# and are a less reliable source of data. set here the maximum
# length of a sentence
#
max-sentence-length = 80

### GIZA++ does not allow sentence pairs of highly uneven length.
# since uneven sentence length is an indicator of a misalignment,
# we set a maximum ratio of 3 (this also gives us room for compoudn splitting)
#
cleaner = "$moses-script-dir/training/clean-corpus-n.perl -ratio 3"

[CORPUS:parallelA]
raw-stem = $toy-data/parallelA.$pair-extension

[CORPUS:parallelB]
raw-stem = $toy-data/parallelB.$pair-extension

[CORPUS:parallelC]
# if you do your own parsing (or wanna re-use other data, like http://statmt.org/rsennrich/parsed_wmt/ ),
# you can add parsed corpora to your system like this
clean-parsed-stem = $toy-data/parallelC.$pair-extension.parsed_target

#################################################################
# LANGUAGE MODEL TRAINING

[LM]

cores = 10

### tool to be used for language model training
# for instance: ngram-count (SRILM), train-lm-on-disk.perl (Edinburgh) 
# 
lm-training = $srilm-dir/ngram-count
settings = "-interpolate -kndiscount -unk"
order = 5

### script to use for binary table format
# (default: no binarization)
#
#lm-binarizer = $moses-src-dir/irstlm/bin/compile-lm

# kenlm, also set type to 8
#lm-binarizer = $moses-src-dir/kenlm/build_binary
#type = 8

### script to create quantized language model format
# (default: no quantization)
# 
#lm-quantizer = $moses-src-dir/irstlm/bin/quantize-lm

### tools to use to prepare the data
#
#tokenizer = 
#lowercaser = 

### each language model to be used has its own section here

### if corpus preparation should be skipped, 
# point to the prepared language model
#
#lm = 

[LM:parallelA]
raw-corpus = $toy-data/parallelA.$pair-extension.$output-extension

[LM:parallelB]
raw-corpus = $toy-data/parallelB.$pair-extension.$output-extension

[LM:parallelC]
raw-corpus = $toy-data/parallelC.$pair-extension.$output-extension

[LM:monolingualA]
raw-corpus = $toy-data/monolingualA.$output-extension

#################################################################
# INTERPOLATING LANGUAGE MODELS

[INTERPOLATED-LM]

# if multiple language models are used, these may be combined
# by optimizing perplexity on a tuning set
# see, for instance [Koehn and Schwenk, IJCNLP 2008]

### script to interpolate language models
# if commented out, no interpolation is performed
#
script = $moses-script-dir/ems/support/interpolate-lm.perl

### tuning set
# you may use the same set that is used for mert tuning (reference set)
#
raw-tuning = $toy-data/newstest2012.$output-extension

### script to use for binary table format for irstlm or kenlm
# kenlm, also set type to 8
lm-binarizer = $moses-src-dir/bin/build_binary
type = 8

#################################################################
# TRANSLATION MODEL TRAINING

[TRAINING]

### training script to be used: either a legacy script or 
# current moses training script (default) 
# 
script = $moses-script-dir/training/train-model.perl

### general options
#
training-options = "-mgiza -mgiza-cpus 8 -sort-buffer-size 10G -sort-compress gzip -cores 16 -alt-direct-rule-score-2 --ghkm-tree-fragment"

### symmetrization method to obtain word alignments from giza output
# (commonly used: grow-diag-final-and)
#
alignment-symmetrization-method = grow-diag-final-and

run-giza-in-parts = 5

### if word alignment (giza symmetrization) should be skipped,
# point to word alignment files
#
# word-alignment =

### hierarchical rule set
#
hierarchical-rule-set = true
use-ghkm = true
use-pcfg-feature = true
use-unknown-word-soft-matches = true
dont-tune-glue-grammar = true

extract-settings = "--UnknownWordMinRelFreq 0.01 --MaxNodes 40 --MaxRuleDepth 7 --MaxRuleSize 7 --AllowUnary"
score-settings = " --GoodTuring --LowCountFeature --MinCountHierarchical 2 --MinScore 2:0.0001"


### if phrase extraction should be skipped,
# point to stem for extract files
#
# extracted-phrases = 

### if phrase table training should be skipped,
# point to phrase translation table
#
# phrase-translation-table = 

### if training should be skipped, 
# point to a configuration file that contains
# pointers to all relevant model files
# config =

####################################################### TUNING: finding good weights for model components

[TUNING]

### instead of tuning with this setting, old weights may be recycled
# specify here an old configuration file with matching weights
#
#weight-config =

### tuning script to be used
#
tuning-script = $moses-script-dir/training/mert-moses.pl
tuning-settings = "-mertdir $moses-src-dir/bin --batch-mira --return-best-dev -maximum-iterations 25 --threads 16 -batch-mira-args='--sctype BLEU,HWCM'"

### specify the corpus used for tuning 
# it should contain 100s if not 1000s of sentences
#
raw-input = $toy-data/newstest2012.$input-extension
# tokenized-input = 
# factorized-input = 
# input =

inputtype = 3

raw-reference = $toy-data/newstest2012.$output-extension
# tokenized-reference = 
# factorized-reference = 
# reference = 

### size of n-best list used (typically 100)
#
nbest = 1000

### ranges for weights for random initialization
# if not specified, the tuning script will use generic ranges
# it is not clear, if this matters
#
# lambda = 

### additional flags for the decoder
#
decoder-settings = "-feature-overwrite 'TranslationModel0 table-limit=100' -threads 8 -max-chart-span 50 -rule-limit 50 -n-best-trees"

### if tuning should be skipped, specify this here
# and also point to a configuration file that contains
# pointers to all relevant model files
#


#########################################################
## RECASER: restore case, this part only trains the model

[RECASING]

#decoder = $moses-src-dir/moses-cmd/src/moses.1521.srilm

### training data
# raw input needs to be still tokenized,
# also also tokenized input may be specified
#
#tokenized = [LM:europarl:tokenized-corpus]

# recase-config = 

#lm-training = $moses-src-dir/srilm/bin/i686/ngram-count

#######################################################
## TRUECASER: train model to truecase corpora and input

[TRUECASER]

### script to train truecaser models
#
trainer = $moses-script-dir/recaser/train-truecaser.perl

### training data
# raw input needs to be still tokenized,
# also also tokenized input may be specified
#
# tokenized-stem = $working-dir/data/ep+nc

### trained model
#
#truecase-model = 

############################################################
## EVALUATION: translating a test set using the tuned system

[EVALUATION]

### number of jobs (if parallel execution of testing)
#
jobs = 10

filter-settings = "  "


### prepare system output for scoring 
# this may include detokenization and wrapping output in sgm 
# (needed for nist-bleu, ter, meteor)
#
#recaser = $moses-script-dir/recaser/recase.perl
wrapping-script = "$moses-script-dir/ems/support/wrap-xml.perl $output-extension"
# output-sgm = 

### should output be scored case-sensitive (default: no)?
#
# case-sensitive = yes

### BLEU
#
nist-bleu = $moses-script-dir/generic/mteval-v13a.pl
nist-bleu-c = "$moses-script-dir/generic/mteval-v13a.pl -c"
# multi-bleu = $edinburgh-script-dir/multi-bleu.perl
# ibm-bleu =

### TER: translation error rate (BBN metric) based on edit distance
#
# ter = $edinburgh-script-dir/tercom_v6a.pl

### METEOR: gives credit to stem / worknet synonym matches
#
# meteor = 

### Analysis: carry out various forms of analysis on the output
#
analysis = $moses-script-dir/ems/support/analysis.perl
#analyze-coverage = yes
report-segmentation = yes


[EVALUATION:newstest2013]
decoder-settings = "-feature-overwrite 'TranslationModel0 table-limit=100' -threads 8 -max-chart-span 50 -rule-limit 100"
input-sgm = $toy-data/newstest2013-src.$input-extension.sgm
wrapping-frame = $input-sgm
reference-sgm = $toy-data/newstest2013-ref.$output-extension.sgm

[REPORTING]

### what to do with result (default: store in file evaluation/report)
# 
# email = pkoehn@inf.ed.ac.uk

